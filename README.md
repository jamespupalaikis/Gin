# Gin Rummy with Deep Q-Learning

## Introduction
Q-learning is a reinforcement learning algorithm that seeks to build a procedural model to complete a task, by assigning values to each available move, given a certain "state"  of that task. It is especially effective in creating a model that can effectively play a game, as states, turns, and "value" of a given turn are clear and discrete. A high value move in this context would be one associated with a high probability of eventually winning a game. The q-learning algorithm uses the Bellman equation to assign a value to a move based on the eventual outcome of the game, positively reinforcing it if it leads to a victory, and vice-versa. This algorithm is not limited to a specific model; it can be done as simply as creating a tabular representation of move value given a board state, or as exotic as relying on a neural network to attempt to generalize the pattern of move value. It is the latter approach that interests me,both as an introdction into reinforcement learning, and as an exercise in building machine learning apparatus for custom purposes from the ground up. <br> <br>
I've long held a fondness for the game of gin-rummy. It is a game that despite involving random card draws as a core mechanic, allows for extreme expression of skill to the point where an expert will almost always badly defeat someone of lesser skill. When I began this project, it was the simplicity of the game paired with this prior fondness that drew me to gin-rummy as the game of choice to train a deep q-learning model to play, however serendipidously it ended up being a great match for the algorithm, for reasons I will address later. For those unfamiliar, the rules of the game can be found [here.](https://bicyclecards.com/how-to-play/gin-rummy/)

## The Q-Learning Model

When planning my approach, I quickly realized that there would be a few hurdles that would need to be overcome to adapt the algorithm to my game of choice. Firstly, it became obvious that one neural network would not be sufficient. In gin, there are two separate types of decisions that need to be made, after the initial draw: One needs to decide which deck to draw from, and then subsequently pick a card in hand to discard. I decided to utilize 3 neural networks for this reason, independently trained. The first network, the "start network" would handle exclusively the initial turn, which is unique in its outcomes. It would be a simple binary classification network that would decide whether to draw the faceup card or pass. The second network, the "draw network", would be very similar, choosing between the faceup card in deck 1 and the facedown card in deck 2. Predictably, the final network would handle which card to discard from the deck. 

### Network Parameters:
Each network takes in some number of 4x13 matrix channels. Each of these sparse matrices represents a grouping of cards, rows representing suits, columns values, and a 1 representing the presence of a certain card. This complements the structure of the convolutional neural network well; The network will be able to take in information about the state, and due to the adjacency of cards that form runs and sets in this format, the convolutional layers are able to learn concepts of which moves are valuable efficiently. 

<p>
  The start networks and draw networks are very similar, both in structure and in purpose. The only differences between the two (other than the training data) is the loss function, and the game consequences of not drawing the faceup card. I strongly considered merging these into a single network (and still am) but I ultimately decided to keep them separate due to the expected result being the draw network training overwhelming the other data by sheer volume.  I will now refer to the draw network only, as it is functionally the same as the start network.  

<br> 
The network takes in 3 channels of matrices, representing the faceup card, the cards that have been discarded, and the player's hand. The loss function is It has a single output within (0,1), with a 1 representing the facedown deck and a 0 being a draw from the faceup pile. Each turn is stored when played, and the value trained on is calculated as 0.5 +  (points*mult *move)/(129 * 2), where points is the final score of the game (negative if the opponent won), mult is the penalty coefficient obtained from Bellman's equation equal to 0.965^(n) for the nth from last turn of the game, move is a number from  [-1,1] which takes -1 if the move decided by the network was the facedown pile and 1 if the network drew from the facup pile, and 129 is the maximum score for a game given the ruleset used. This labelling equation seems overly complicated, but it guarantees the "correct" answer that the network trains upon encourages the same move if the network won, and a different move if the result was a loss. It ends up being proportional to the final score, i.e. if the opponent wins with a maximum score of 129, and for a given boardstate the network returned a move <0.5, the training label will be  0.5 + 0.5*multiplier.Conversely, if the network returns a move >0.5 for a boardstate, and the final result is a win with a score of 129//2, the training label will be 0.5 + multiplier*0.25. This training protocol almost guarantees a massive amount of data will be needed to properly train the ntowrk. A game may still result in a loss even if some of the moves were correct (and in fact, given the stochastic nature of the game, a perfectly played game can still be a loss!) , however the proper turn valuation should emerge over a large number of games. 
</p>
 <br> <br> 
 <p>
  The neural network that handles discards is quite a bit different from the other two in a few ways. It takes in 

<br>  
  
</p>


